---
title: "4: A model report"
format: 
  html:
    fig-height: 3
editor: source
---

```{r, message=FALSE, warning = FALSE}
library(tidyverse)
library(cmdstanr)

source("../../functions/import_data.R")
source("../../functions/prep_data.R")
source("../../functions/compute_summary_stats.R")
source("../../functions/plot_model.R")
source("../../functions/plot_data.R")
source("../../functions/post_functions.R")
source("../../functions/sim_foraging_data.R")

options(mc.cores = 1, digits = 2)

# set global ggplot theme
theme_set(ggthemes::theme_tufte())
```

these are fit by script `fit_all_models.R` and run on our cluster/big-chungus.

We then compute accuracy for each model using script `2_`

Here we provide more in depth analysis on how well a particular model fits a given dataset


```{r}
model_ver <- "1_2"
dataset   <- "kristjansson2014plos"
```


```{r}
# read in data
d <- import_data(dataset)

# read in model and predictions for test data
m <- read_rds(paste0("scratch/", dataset, "_train_", model_ver, ".model"))
t <- read_rds(paste0("scratch/", dataset, "_test_", model_ver, ".model"))
```

# How well does the model fit?

## Prediction Accuracy

Compute chance base-rates for each dataset
```{r}
d$stim %>% group_by(person, trial) %>%
  summarise(n = n(), .groups = "drop") -> dn
  
items <- seq(1, unique(dn$n))
items_left <- unique(dn$n) - items + 1
  
n_conditions <- length(levels(d$stim$condition))
  
baseline <- tibble(found = 1:unique(dn$n),
                   accuracy = (1/items_left))
  
rm(items, items_left)
```


```{r}
#| label: compute-all_acc
#| cache: true

pred <- summarise_postpred(list(training = m, testing = t), d, 
                                 get_sim = FALSE, draw_sample_frac=0.25)
acc <- compute_acc(pred$acc)

acc %>% ggplot(aes(found, accuracy)) +
  geom_lineribbon(aes(fill = condition, colour = condition,
                     ymin = .lower, ymax = .upper,
                  group = interaction(condition, .width)), 
                  alpha = 0.5) +
  geom_path(data = baseline, linetype = 2, colour = "black") +
  facet_grid(.~split) +
  scale_color_viridis_d() + 
  scale_fill_viridis_d()
```

## ELPD? 

# Model checking

Did the mcmc sampling work ok? 

```{r}

# diagnostic checks
# m$diagnostic_summary()
# 
# traceplots
 # bayesplot::mcmc_trace(m$draws(),
 #                      pars = c("b_a[1]", "b_a[2]", "b_stick[1]", "b_stick[2]", 
 #                                "rho_delta[1]", "rho_delta[2]"),  #, "rho_psi[1]", "rho_psi[2]"
 #                      facet_args = list(nrow = 2))
```


# Posterior Densities

```{r}
#| warning: false
#| echo: false


post <- extract_post(m, d)
plot_model_fixed(post)
```

# Posterior Predictions

Compare run statistics and inter-item-selection-vectors.

```{r}
read_csv(paste0("scratch/run_statistics", model_ver, ".csv"),
         show_col_types = FALSE) %>%
  filter(dataset == {{dataset}}) %>%
  ggplot(aes(observed, predicted, colour = condition))  +
  geom_point() +
  geom_abline(linetype = 2) +
  scale_x_continuous("observed run length") + 
  scale_y_continuous("predicted run length") + 
  ggtitle("Run Statistics") -> plt_rl
```

```{r}
read_csv(paste0("scratch/iisv_statistics", model_ver, ".csv"),
                 show_col_types = FALSE) %>%
  filter(dataset == {{dataset}}) %>%
  mutate(d = sqrt(d2)) %>%
  group_by(condition, found, x) %>%
  median_hdci(d, .width = c(0.53, 0.97)) %>%
  ggplot(aes(found, y = d, ymin = .lower, ymax = .upper, 
             fill = x, group = interaction(x, .width)))  +
  geom_path(aes(colour = x)) +
  geom_ribbon(alpha = 0.33) +
  facet_wrap(condition ~ .) +
  theme(legend.title = element_blank()) -> plt_iisv_delta
```

```{r}
plt_rl + plt_iisv_delta
```


---
title: "3_fitting_original_model_to_old_datasets"
format: html
editor: visual
---

```{r, message=FALSE, warning = FALSE}
library(tidyverse)
library(cmdstanr)

source("../../functions/import_data.R")
source("../../functions/prep_data.R")
source("../../functions/compute_summary_stats.R")
source("../../functions/plot_model.R")
source("../../functions/plot_data.R")
source("../../functions/post_functions.R")
source("../../functions/sim_foraging_data.R")

options(mc.cores = 1)

# set global ggplot theme
theme_set(ggthemes::theme_tufte())
```

# Fitting Model to Previously Collected Datasets

some text...

these are fit by scripts xxx.R and run on our cluster.

## Clarke2022qjep

## Tagu2022cog

```{r}

d <- import_data('tagu2022cog')

```

```{r, eval = FALSE}
d_list <- prep_data_for_stan(d$found, d$stim, c("spatial", "item_class"))
d_list <- add_priors_to_d_list(d_list)

```

```{r, eval = FALSE}

iter = 100
d_list$n_trials_to_sim <- 1

mod <- cmdstan_model("../../models/multi_level/FoMo1_0.stan", 
                     cpp_options = list(stan_threads = TRUE), force_recompile = TRUE)

fit <- mod$sample(data = d_list, 
                 chains = 4, parallel_chains = 4, threads = 4,
                 refresh = 10, 
                 init = 1,
                 iter_warmup = iter, iter_sampling = iter,
                 sig_figs = 3)

fit$save_object("scratch/tagu_1_0_tmp.rds")

# pre save moment match loo
fit1_0_loo <- fit$loo(moment_match = TRUE)
saveRDS(fit1_0_loo, "scratch/tagu_1_0_loo.rds")


```

```{r, eval = FALSE}

mod1_1 <- cmdstan_model("../../models/multi_level/FoMo1_1.stan", 
                     cpp_options = list(stan_threads = TRUE), force_recompile = TRUE)

fit1_1 <- mod1_1$sample(data = d_list, 
                 chains = 4, parallel_chains = 4, threads = 4,
                 refresh = 10, 
                 init = 1,
                 iter_warmup = iter, iter_sampling = iter,
                 sig_figs = 3)

fit1_1$save_object("scratch/tagu_1_1_tmp.rds")

# pre save moment match loo
fit1_1_loo <- fit1_1$loo(moment_match = TRUE)
saveRDS(fit1_1_loo, "scratch/tagu_1_1_loo.rds")

```

### Model checking

```{r}

fit1_0 <- readRDS("scratch/tagu_1_0_tmp.rds")
fit1_1 <- readRDS("scratch/tagu_1_1_tmp.rds")

# diagnostic checks
fit1_0$diagnostic_summary()
fit1_1$diagnostic_summary()

# traceplots
bayesplot::mcmc_trace(fit1_0$draws(), pars = c("b_a[1]", "b_a[2]", "b_stick[1]", "b_stick[2]", 
                                               "rho_delta[1]", "rho_delta[2]", "rho_psi[1]", "rho_psi[2]"))

bayesplot::mcmc_trace(fit1_1$draws(), pars = c("b_a[1]", "b_a[2]", "b_stick[1]", "b_stick[2]", 
                                               "rho_delta[1]", "rho_delta[2]", "rho_psi[1]", "rho_psi[2]"))

```

### Which model fits best - training set acc


```{r}
chance_acc <- mean(1/(seq(53, 1)))


pred <- summarise_postpred(fit1_0, d, get_sim = FALSE, draw_sample_frac=1)
acc10 <- compute_training_acc(pred$acc) %>% mutate(model = "FoMo 1.0")


pred <- summarise_postpred(fit1_1, d, get_sim = FALSE, draw_sample_frac=1)
acc11 <- compute_training_acc(pred$acc) %>% mutate(model = "FoMo 1.1")
  

bind_rows(acc10, acc11) %>%
  ggplot(aes(y = condition, x = accuracy, xmin = .lower, xmax = .upper, colour = model )) +
  geom_pointinterval(position = position_dodge(width = 0.25))
```

### Which model fits best - LOO?



```{r}

fit1_0_loo <- readRDS("scratch/tagu_1_0_loo.rds")
fit1_1_loo <- readRDS("scratch/tagu_1_1_loo.rds")

knitr::kable(loo::loo_model_weights(list(
  "model 1.1" = fit1_1_loo,
  "model 1.0" = fit1_0_loo)))


```

This suggests that model 1.1 fits best.

### Plotting the model

```{r}

# extract post
post <- extract_post(fit1_1, d)

# plot model
# things to think about - rho delta here is squished because the scale is changed. I think this is because the prior is a bit wrong.
plot_model_fixed(post)
plot_model_random(post)

```


# OLD STUFF BELOW HERE

```{r}
dataset <- "tagu2022cog"

d <- import_data(dataset)  

pp_folder <- paste0("../../scratch/", dataset, "/postpred/")

pp1.1 <- read_csv(paste0(pp_folder, "post_pred_item_summary_1.1.csv"))
pp1.2 <- read_csv(paste0(pp_folder, "post_pred_item_summary_1.2.csv"))
pp1.3 <- read_csv(paste0(pp_folder, "post_pred_item_summary_1.3.csv"))
  
# join prediction dataframes together
full_join(
  pp1.1 %>% mutate(model = "1.1"),
  pp1.2 %>% mutate(model = "1.2"))  %>%
  full_join(pp1.3 %>% mutate(model = "1.3")) -> pp

# remove uneeded dataframs
rm(pp1.1, pp1.2, pp1.3)

# we're not modelling the first selection, so remove found == 1

pp <- filter(pp, found != 1)
```

```{r fig.cap="comparision of model variants. Very litte uncertainty over multiple draws."}
n_targ = 54

ggplot(pp, aes(found, W)) + 
  geom_line(aes(colour = model,  group = interaction(draw, model)), alpha = 0.1) + 
  geom_line(data = tibble(x = 1:n_targ, y = 1/(n_targ-x+1)), aes(x, y)) + 
  theme_bw() +
  facet_wrap(~condition, nrow = 1)
```

```{r}
pp %>% select(-acc) %>% 
  filter(found %in% 2:39) %>%
  group_by(model) %>%
  mean_hdci(W) %>%
  knitr::kable()

pp %>% select(-W) %>% 
  filter(found %in% 2:39) %>%
  group_by(model) %>%
  mean_hdci(acc) %>%
  knitr::kable()

rm(pp)
```

After comparisons, select the best model

### Model Diagnostics

First, import our fitted model

```{r}
m <- readRDS("../../scratch/tagu2022cog/models/fit_model1.3.RDS")

m_summary <- m$summary()
```

#### Rhat Statistics

What is current best practise around this? Reporting standards? Worry thresholds?

Are there guidelines here: https://discourse.mc-stan.org/t/summarising-rhat-values-over-multiple-variables-fits/23957/5

```{r}
ggplot(m_summary, aes(rhat)) + geom_histogram()
```

Extract parameters that have Rhat \> 1.01

```{r}
filter(m_summary, rhat > 1.01) %>%
  knitr::kable(digits = 3)
```

### Pairs Plot

```{r}
bayesplot::mcmc_pairs(m$draws(format = "df"), 
                      pars = c("bA[1]", "bA[2]", "b_stick[1]", "b_stick[2]",
                               filter(m_summary, rhat>1.009)$variable))
```

### Posterior Density Plots

```{r}
cl <-  levels(d$stim$condition)
post <- extract_post(m, d$stim, cl, 8000)
```

#### Fixed Effects

```{r, fig.cap="fixed effects"}
plot_model_fixed(post, m, d$stim, cl)
```

#### Random Effects

```{r, fig.cap="random effects"}
plot_model_random(post$var)
```

### Individaul Differences

```{r, fig.cap="individual differences"}
# plot_model_individual(post$random)

post$random %>%
  select(-.draw) %>%
  pivot_longer(-c(person, condition), names_to = "param", values_to = "value") %>%
  group_by(person, condition, param) %>%
  summarise(value = median(value)) %>%
  pivot_wider(names_from = "param", values_from = "value") %>%
  mutate(uA = plogis(uA),
         u_stick = plogis(u_stick)) -> pltdat

pltvar <- function(var1, var2) {
  ggplot(pltdat, aes(.data[[var1]], .data[[var2]], 
                     colour = condition)) + geom_point()
}

(pltvar("uA", "u_stick") + pltvar("uA", "u_delta") + pltvar("uA", "u_psi")) /
  (pltvar("u_stick", "u_delta") + pltvar("u_stick", "u_psi") + pltvar("u_delta", "u_psi")) +
  plot_layout(guides="collect")
```

### A Sanity check

Let us sanity check the person with the v high u_psi value

```{r}
filter(pltdat, u_psi > 2)

pp = 19
trl = 350

trl_stim <- filter(d$stim, person == pp, condition == 2, trial == trl)
trl_found <- filter(d$found, person == pp, condition == 2, trial == trl)

plot_a_trial(trl_stim, trl_found)

```

### Do individual param values tell us anything about run lengths?

```{r}
rl <- get_run_info_over_trials(d$found) %>% 
  group_by(person, condition) %>%
  summarise(max_run_length = mean(max_run_length))

post$random %>%
  group_by(condition, person) %>%
  median_hdci(uA, u_stick) %>%
  full_join(rl, by = join_by(condition, person)) -> rl

ggplot(rl, aes(uA, xmin = uA.lower, xmax = uA.upper, y = max_run_length, colour = condition)) + 
  geom_errorbarh() -> plt_uA

ggplot(rl, aes(u_stick, xmin = u_stick.lower, xmax = u_stick.upper, y = max_run_length, colour = condition)) + 
  geom_errorbarh() -> plt_stick

plt_uA + plt_stick + plot_layout(guides = "collect")
```

### Posterior Predictions

plot predicated v empirical run stats, inter-targ distances, directions.

For these, we need the trial-level posterior predictions

## Disussion

---
title: "2) fitting the NEW  old foraging model"
author: "A D F Clarke & A Excellent Hughes"
format: html
editor: source
---

```{r, message=FALSE}
library(tidyverse)
library(cmdstanr)

source("../../functions/import_data.R")
source("../../functions/prep_data.R")
source("../../functions/compute_summary_stats.R")
source("../../functions/plot_model.R")
source("../../functions/plot_data.R")
source("../../functions/post_functions.R")
source("../../functions/sim_foraging_data.R")

options(mc.cores = 4)

# set global ggplot theme
theme_set(ggthemes::theme_tufte())
```

# Fitting Model to Simulated Data

## Example 1 - simple model with one participant

### First simulate some data

```{r}

n_trials_per_cond <- 10

n_item_class <- 2
n_item_per_class <- 20
item_class_weights = c(0.7, 0.3, 0, 0)
b_stick = 2
b_memory = 0

abs_dir_tuning = list(kappa = rep(20, 4), theta = c(2, 0.5, 1, 0.5))
rho_delta = 10
rho_psi = 5

d <- sim_foraging_multiple_trials(person = 1, 
                                  condition = "test",
                                  n_item_class =  n_item_class, n_item_per_class = n_item_per_class,
                                  item_class_weights = item_class_weights, item_labels = item_labels,
                                  b_stick = b_stick, 
                                  rho_delta = rho_delta, 
                                  rho_psi = rho_psi, 
                                  abs_dir_tuning = abs_dir_tuning,
                                  b_memory = b_memory,
                                  inital_sel_params = inital_sel_params,
                                  init_sel_lambda = init_sel_lambda)


```

### Fitting model 1.0

```{r}

iter = 100
mod <- cmdstan_model("../../models/simple/FoMo1_0.stan", 
                     cpp_options = list(stan_threads = TRUE))

d_list <- prep_data_for_stan(d$found, d$stim, c("spatial", "item_class"))

# add priors to list
d_list$prior_mu_b_a <- 0
d_list$prior_sd_b_a <- 0.5
d_list$prior_mu_b_stick <- 0
d_list$prior_sd_b_stick <- 1
d_list$prior_mu_rho_delta <- 15
d_list$prior_sd_rho_delta <- 5
d_list$prior_mu_rho_psi <- 0
d_list$prior_sd_rho_psi <- 1
d_list$n_trials_to_sim <- 10

# run model
m <- mod$sample(data = d_list, 
                  chains = 4, parallel_chains = 4, threads = 4,
                  refresh = 10, 
                  iter_warmup = iter, iter_sampling = iter,
                  sig_figs = 3)

```

### Extract posterior

```{r}

# extract post
post <- extract_post(m, d, multi_level = FALSE)

```

### Plot model

```{r}

# plot model
plot_model_fixed(post, gt = list(b_a = plogis(item_class_weights[1]),
                                 b_stick = b_stick,
                                 rho_delta = rho_delta,
                                 rho_psi = rho_psi))

```

### Check predictions

```{r}

pred <- summarise_postpred(m, d)

plot_model_accuracy(pred)

```

### Plot comparison between a real and a simulated trial

```{r}

# plot comparison between a real and simulated trial

pltreal <- plot_a_trial(d$stim, d$found, 1)
pltsim <- plot_a_trial(d$stim, pred$sim %>% filter(.draw == 1), trial = 1)

pltreal + pltsim


```
## Example 2 - multilevel model

### First simulate some data

```{r simdata1, cache=TRUE}
item_class_weights = list(c(0.7, 0.3, 0, 0))

b_stick = 1

rho_delta = 15
sd_rho_delta = 5

rho_psi = -1

abs_dir_tuning = list(kappa = rep(10, 4), theta = rep(1, 4))

# initial bias params
inital_sel_params <- tibble(
  a1x = 2,
  b1x = 2,
  a2x = 1,
  b2x = 10,
  a1y = 2,
  b1y = 2,
  a2y = 10,
  b2y = 1) 

d <- sim_foraging_people(n_people = 5,
                    n_conditions = 1,
                    cond_lab = c("simple test"),
                    n_trials_per_cond = 4,
                    n_item_class = 2, n_item_per_class = 20,
                    item_class_weights, sd_bA = 0.2,
                    b_stick = b_stick, sd_b_stick = 1,
                    rho_delta = rho_delta, sd_rho_delta = sd_rho_delta,
                    rho_psi = rho_psi, sd_rho_psi = 0.5,
                    abs_dir_tuning = abs_dir_tuning,
                    inital_sel_params = inital_sel_params) 

d$found <- fix_person_and_trial(d$found)
d$stim <- fix_person_and_trial(d$stim)
```

### Now fit our model

We will fit model 1.0 (basically, the corrected original model)

```{r prepsimdata1}
d_list <- prep_data_for_stan(d$found, d$stim, c("spatial", "item_class"))
d_list <- add_priors_to_d_list(d_list)
```


```{r fitmodelsimdata1, eval = FALSE}
iter = 100
mod <- cmdstan_model("../../models/multi_level/FoMo1_0.stan", 
                     cpp_options = list(stan_threads = TRUE))

fit <- mod$sample(data = d_list, 
                 chains = 4, parallel_chains = 4, threads = 4,
                 refresh = 10, 
                 iter_warmup = iter, iter_sampling = iter,
                 sig_figs = 3)

fit$save_object("scratch/multi_level_tmp.rds")

```

```{r}

fit <- readRDS("scratch/multi_level_tmp.rds")

```

### Posterior Density Plots

```{r extractsim1post, cache=TRUE}
post <- extract_post(fit, d)
```

#### Fixed Effects

```{r plotsim1fixed, fig.cap="fixed effects"}
plot_model_fixed(post, gt = list(b_a = plogis(item_class_weights[[1]][1]),
                                 b_stick = b_stick,
                                 rho_delta = rho_delta,
                                 rho_psi = rho_psi))
```

#### Random Effects

```{r plotsim1random, fig.cap="random effects"}
plot_model_random(post)
```


## Example 2

## Discussion

Hopefully we agree that the model fits well

# Fitting Model to Previously Collected Datasets

 some text...

these are fit by scripts xxx.R and run on our cluster.


## Clarke2022qjep

## Tagu2022cog

### Traceplots

Show traceplots for key params and/or any with large Rhat? 

### Which model fits best?

Maybe better described as: how well do the models fit the training dataset? (When we collect new data later on, we will split the data and do a training and a test dataset).

```{r}
dataset <- "tagu2022cog"

d <- import_data(dataset)  

pp_folder <- paste0("../../scratch/", dataset, "/postpred/")

pp1.1 <- read_csv(paste0(pp_folder, "post_pred_item_summary_1.1.csv"))
pp1.2 <- read_csv(paste0(pp_folder, "post_pred_item_summary_1.2.csv"))
pp1.3 <- read_csv(paste0(pp_folder, "post_pred_item_summary_1.3.csv"))
  
# join prediction dataframes together
full_join(
  pp1.1 %>% mutate(model = "1.1"),
  pp1.2 %>% mutate(model = "1.2"))  %>%
  full_join(pp1.3 %>% mutate(model = "1.3")) -> pp

# remove uneeded dataframs
rm(pp1.1, pp1.2, pp1.3)

# we're not modelling the first selection, so remove found == 1

pp <- filter(pp, found != 1)
```


```{r fig.cap="comparision of model variants. Very litte uncertainty over multiple draws."}
n_targ = 54

ggplot(pp, aes(found, W)) + 
  geom_line(aes(colour = model,  group = interaction(draw, model)), alpha = 0.1) + 
  geom_line(data = tibble(x = 1:n_targ, y = 1/(n_targ-x+1)), aes(x, y)) + 
  theme_bw() +
  facet_wrap(~condition, nrow = 1)
```


```{r}
pp %>% select(-acc) %>% 
  filter(found %in% 2:39) %>%
  group_by(model) %>%
  mean_hdci(W) %>%
  knitr::kable()

pp %>% select(-W) %>% 
  filter(found %in% 2:39) %>%
  group_by(model) %>%
  mean_hdci(acc) %>%
  knitr::kable()

rm(pp)
```

After comparisons, select the best model

### Model Diagnostics

First, import our fitted model

```{r} 
m <- readRDS("../../scratch/tagu2022cog/models/fit_model1.3.RDS")

m_summary <- m$summary()
```

#### Rhat Statistics

What is current best practise around this? Reporting standards? Worry thresholds? 

Are there guidelines here: https://discourse.mc-stan.org/t/summarising-rhat-values-over-multiple-variables-fits/23957/5

```{r}
ggplot(m_summary, aes(rhat)) + geom_histogram()
```

Extract parameters that have Rhat > 1.01

```{r}
filter(m_summary, rhat > 1.01) %>%
  knitr::kable(digits = 3)
```

### Pairs Plot

```{r}
bayesplot::mcmc_pairs(m$draws(format = "df"), 
                      pars = c("bA[1]", "bA[2]", "b_stick[1]", "b_stick[2]",
                               filter(m_summary, rhat>1.009)$variable))
```

### Posterior Density Plots

```{r}
cl <-  levels(d$stim$condition)
post <- extract_post(m, d$stim, cl, 8000)
```

#### Fixed Effects

```{r, fig.cap="fixed effects"}
plot_model_fixed(post, m, d$stim, cl)
```

#### Random Effects

```{r, fig.cap="random effects"}
plot_model_random(post$var)
```

### Individaul Differences

```{r, fig.cap="individual differences"}
# plot_model_individual(post$random)

post$random %>%
  select(-.draw) %>%
  pivot_longer(-c(person, condition), names_to = "param", values_to = "value") %>%
  group_by(person, condition, param) %>%
  summarise(value = median(value)) %>%
  pivot_wider(names_from = "param", values_from = "value") %>%
  mutate(uA = plogis(uA),
         u_stick = plogis(u_stick)) -> pltdat

pltvar <- function(var1, var2) {
  ggplot(pltdat, aes(.data[[var1]], .data[[var2]], 
                     colour = condition)) + geom_point()
}

(pltvar("uA", "u_stick") + pltvar("uA", "u_delta") + pltvar("uA", "u_psi")) /
  (pltvar("u_stick", "u_delta") + pltvar("u_stick", "u_psi") + pltvar("u_delta", "u_psi")) +
  plot_layout(guides="collect")
```

### A Sanity check 

Let us sanity check the person with the v high u_psi value

```{r}
filter(pltdat, u_psi > 2)

pp = 19
trl = 350

trl_stim <- filter(d$stim, person == pp, condition == 2, trial == trl)
trl_found <- filter(d$found, person == pp, condition == 2, trial == trl)

plot_a_trial(trl_stim, trl_found)

```

### Do individual param values tell us anything about run lengths?


```{r}
rl <- get_run_info_over_trials(d$found) %>% 
  group_by(person, condition) %>%
  summarise(max_run_length = mean(max_run_length))

post$random %>%
  group_by(condition, person) %>%
  median_hdci(uA, u_stick) %>%
  full_join(rl, by = join_by(condition, person)) -> rl

ggplot(rl, aes(uA, xmin = uA.lower, xmax = uA.upper, y = max_run_length, colour = condition)) + 
  geom_errorbarh() -> plt_uA

ggplot(rl, aes(u_stick, xmin = u_stick.lower, xmax = u_stick.upper, y = max_run_length, colour = condition)) + 
  geom_errorbarh() -> plt_stick

plt_uA + plt_stick + plot_layout(guides = "collect")
```

### Posterior Predictions

plot predicated v empirical run stats, inter-targ distances, directions. 

For these, we need the trial-level posterior predictions

## Disussion




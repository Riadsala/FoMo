---
title: "4: A model report"
format: 
  html:
    fig-height: 3
editor: source
---

```{r, message=FALSE, warning = FALSE}
library(tidyverse)
library(cmdstanr)

source("../../functions/import_data.R")
source("../../functions/prep_data.R")
source("../../functions/compute_summary_stats.R")
source("../../functions/plot_model.R")
source("../../functions/plot_data.R")
source("../../functions/post_functions.R")
source("../../functions/sim_foraging_data.R")

options(mc.cores = 1, digits = 2)

# set global ggplot theme
theme_set(ggthemes::theme_tufte())
```

these are fit by script `fit_all_models.R` and run on our cluster/big-chungus.

We then compute accuracy for each model using script `2_`

Here we provide more in depth analysis on how well a particular model fits a given dataset


```{r}
model_ver <- "1_3"
dataset   <- "kristjansson2014plos"
```


```{r}
# read in data
d <- import_data(dataset)

# read in model and predictions for test data
m <- read_rds(paste0("scratch/", dataset, "_train_", model_ver, ".model"))
t <- read_rds(paste0("scratch/", dataset, "_test_", model_ver, ".model"))
```

We want to compare to one of the other models

```{r}
model_ver_comparison <- "1_2"

# read in model and predictions for test data
mc <- read_rds(paste0("../1_test_old_model_old_data/scratch/", dataset, "_train_", model_ver_comparison, ".model"))
tc <- read_rds(paste0("../1_test_old_model_old_data/scratch/", dataset, "_test_", model_ver_comparison, ".model"))
```


# How well does the model fit?

## Prediction Accuracy

Compute chance base-rates for each dataset

```{r}
d$stim %>% group_by(person, trial) %>%
  summarise(n = n(), .groups = "drop") -> dn
  
items <- seq(1, unique(dn$n))
items_left <- unique(dn$n) - items + 1
  
n_conditions <- length(levels(d$stim$condition))
  
baseline <- tibble(found = 1:unique(dn$n),
                   accuracy = (1/items_left))
  
rm(items, items_left)
```

```{r}
#| label: compute-all_acc
#| cache: true

# get pred and acc for 1.3
pred <- summarise_postpred(list(training = m, testing = t), d, 
                                 get_sim = FALSE, draw_sample_frac=0.1)
acc <- compute_acc(pred$acc)

# get pred and acc for comparison
predc <- summarise_postpred(list(training = mc, testing = tc), d, 
                                 get_sim = FALSE, draw_sample_frac=0.1)
accc <- compute_acc(predc$acc)

```

now plot

```{r}

bind_rows(acc %>% mutate(model_version = model_ver),
          accc %>% mutate(model_version = model_ver_comparison)) %>%
  ggplot(aes(found, accuracy)) +
  geom_lineribbon(aes(fill = model_version, colour = model_version,
                     ymin = .lower, ymax = .upper,
                  group = interaction(model_version, .width)), 
                  alpha = 0.5) +
  geom_path(data = baseline, linetype = 2, colour = "black") +
  facet_grid(condition~split) +
  scale_color_viridis_d() + 
  scale_fill_viridis_d()
```


Look at accuracy overt first 30 items (as as their are fewer left, less scope for things to matter)

```{r}
acc <- compute_acc(pred$acc, compute_hpdi = FALSE) %>%
  mutate(model_version = model_ver) 

accc <- compute_acc(predc$acc, compute_hpdi = FALSE) %>%
  mutate(model_version = model_ver_comparison) 

bind_rows(acc, accc) %>%
  filter(found < 30) %>%
  group_by(model_version, condition, split, .draw) %>%
  summarise(accuracy = mean(accuracy)) %>%
  median_hdci(accuracy, .width = c(0.53, 0.97)) %>%
  ggplot(aes(condition, accuracy, ymin = .lower, ymax = .upper, 
             colour = model_version)) +
  geom_interval(alpha = 0.5, position = position_dodge(width = 0.2)) +
  facet_wrap(~split)
```

# Posterior Densities

## Old params

```{r}
#| warning: false
#| echo: false

post <- extract_post(m, d)
plot_model_fixed(post)
```
## New theta params


```{r}
post$absdir %>%
  ggplot(aes(theta, fill = comp)) +
  geom_density(alpha = 0.5) + 
  facet_wrap(~condition) +
  scale_fill_viridis_d()

```

